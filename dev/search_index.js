var documenterSearchIndex = {"docs":
[{"location":"arches/tigerlake/#Tigerlake","page":"Tigerlake","title":"Tigerlake","text":"","category":"section"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"Tigerlake CPUs feature just a single 512-bit-fma unit, and thus their theoretical peak FLOPS are comparable with AVX2 CPUs featuing two 256-bit FMA units, such as Intel's Skylake or AMD's Zen2. The much larger register file that AVX512 provides combined with its comparatively much larger L1 and L2 caches (and no doubt helped by the large out of order buffer) make it comparatively very easy to attain near peak performance on Tigerlake.","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"Statically sized benchmarks vs StaticArrays.jl: (Image: sizedbenchmarks)","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"The SMatrix and MMatrix are the immutable and immutable matrix types from StaticArrays.jl, respectively, while StrideArray.jl and PtrArray.jl are mutable array types with optional static sizing providing by PaddedMatrices.jl. The benchmarks also included jmul! on base Matrix{Float64}, demonstrating the performance of PaddedMatrices's fully dynamic multiplication function.","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"SMatrix were only benchmarked up to size 20x20. As their performance at larger sizes recently increased, I'll increase the size range at which I benchmark them in the future.","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"The fully dynamic multiplication is competitive with MKL and OpenBLAS from around 2x2 to 256x256: (Image: dgemmbenchmarkssmall) Unlike the Cascadelake CPU, it was able to hold on with MKL at least through 2000x2000: (Image: dgemmbenchmarksmedium)","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"The version of OpenBLAS used (0.3.10) didn't support Tigerlake yet. Unlike Cascadelake, where approaching the CPU's peak performance can be challenging, it is easy with Tigerlake: Tigerlake is at least as advanced in all respects related to keeping its execution units fed, and it has half as many FMA-units to feed. Hence it managed to keep pace with MKL over this range.","category":"page"},{"location":"arches/tigerlake/","page":"Tigerlake","title":"Tigerlake","text":"Given that LoopVectorization by itself does better on Cascadelake than PaddedMatrices over the 2-256 size range, it'd be worth comparing Tigerlake with simple @avx for loops as well.","category":"page"},{"location":"broadcasting/#Broadcasting","page":"Broadcasting","title":"Broadcasting","text":"","category":"section"},{"location":"broadcasting/","page":"Broadcasting","title":"Broadcasting","text":"Broadcasting StrideArrays is also fast, e.g. to continue on the random number generation example from earlier, we could quickly calculate a Monte Carlo sample of means of log normally distributed random variables:","category":"page"},{"location":"broadcasting/","page":"Broadcasting","title":"Broadcasting","text":"julia> @benchmark sum(exp.(@StrideArray randn(8,10))) # PaddedMatrices\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     127.652 ns (0.00% GC)\n  median time:      129.033 ns (0.00% GC)\n  mean time:        129.041 ns (0.00% GC)\n  maximum time:     163.491 ns (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     888\n\njulia> @benchmark sum(exp.(@SMatrix randn(8,10))) # StaticArrays\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     678.948 ns (0.00% GC)\n  median time:      690.000 ns (0.00% GC)\n  mean time:        690.399 ns (0.00% GC)\n  maximum time:     847.484 ns (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     153","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To install PaddedMatrices.jl, simply:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"PaddedMatrices\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This library is built on ArrayInterface.jl and LoopVectorization.jl. It is still somewhat experimental, and many features such as good linear algebra support, are still missing. It aims to achieve high performance and provide flexibility, while keeping implementations simple.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Please file issues if you encounter problems or have feature requests.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To create an uninitialized StrideArray, use the constructor StrideArray{T}(undef, size_tuple), e.g.:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"julia> StrideArray{Float64}(undef, (3,4)) |> PaddedMatrices.size\n(3, 4)\n\njulia> StrideArray{Float64}(undef, (StaticInt(3),4)) |> PaddedMatrices.size\n(Static(3), 4)\n\njulia> StrideArray{Float64}(undef, (3,StaticInt(4))) |> PaddedMatrices.size\n(3, Static(4))\n\njulia> StrideArray{Float64}(undef, (StaticInt(3),StaticInt(4))) |> PaddedMatrices.size\n(Static(3), Static(4))","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"If a size is specified by a StaticInt, then that dimension will be statically sized. Otherwise, it will by dynamically sized.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To create one filled with random elements, see the RNG section.","category":"page"},{"location":"arches/haswell/#Haswell","page":"Haswell","title":"Haswell","text":"","category":"section"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"The Haswell CPU benchmarked here is a 1.7 GHz laptop CPU. It features two 256-bit FMA units, which gives it comparable peak FLOPS/cycle to Tigerlake. But, with its smaller caches, fewer and smaller registers necessitating churning over the cache more quickly, and more limited out of order capabilities, it is much more difficult to achieve peak performance on Haswell.","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"Statically sized benchmarks vs StaticArrays.jl: (Image: sizedbenchmarks)","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"The SMatrix and MMatrix are the immutable and immutable matrix types from StaticArrays.jl, respectively, while StrideArray.jl and PtrArray.jl are mutable array types with optional static sizing providing by PaddedMatrices.jl. The benchmarks also included jmul! on base Matrix{Float64}, demonstrating the performance of PaddedMatrices's fully dynamic multiplication function.","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"SMatrix were only benchmarked up to size 20x20. As their performance at larger sizes recently increased, I'll increase the size range at which I benchmark them in the future.","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"The fully dynamic multiplication is competitive with MKL and OpenBLAS from around 2x2 to 256x256: (Image: dgemmbenchmarkssmall) (Image: dgemmbenchmarksmedium)","category":"page"},{"location":"arches/haswell/","page":"Haswell","title":"Haswell","text":"Benchmarks will be added later.","category":"page"},{"location":"arches/cascadelake/#Cascadelake","page":"Cascadelake","title":"Cascadelake","text":"","category":"section"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"Cascadelake CPUs feature 2 512-bit-fma units, and thus can achieve high FLOPS in BLAS-like operations. The particular CPU on which these benchmarks were run had its heavy-AVX512 clock speed set to 4.1 GHz, providing a theoretical peak of 131.2 GFLOPS/core.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"Statically sized benchmarks vs StaticArrays.jl: (Image: sizedbenchmarks)","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"The SMatrix and MMatrix are the immutable and immutable matrix types from StaticArrays.jl, respectively, while StrideArray.jl and PtrArray.jl are mutable array types with optional static sizing providing by PaddedMatrices.jl. The benchmarks also included jmul! on base Matrix{Float64}, demonstrating the performance of PaddedMatrices's fully dynamic multiplication function.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"SMatrix were only benchmarked up to size 20x20. As their performance at larger sizes recently increased, I'll increase the size range at which I benchmark them in the future.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"The fully dynamic multiplication is competitive with MKL and OpenBLAS from around 2x2 to 256x256: (Image: dgemmbenchmarkssmall) However, beyond this size, performance begins to fall behind: (Image: dgemmbenchmarksmedium) OpenBLAS eventually ascends to about 120 GFLOPS, but PaddedMatrices seems stuck at around 100 GFLOPS.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"The packing and blocking need work.","category":"page"},{"location":"arches/cascadelake/","page":"Cascadelake","title":"Cascadelake","text":"Multithreading is coming.","category":"page"},{"location":"stack_allocation/#Stack-Allocation","page":"Stack Allocattion","title":"Stack Allocation","text":"","category":"section"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"Stack allocated arrays are great, as are mutable arrays.","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"PaddedMatrices.jl tries it's hardest to provide you with both. As you may have noted from the RNG and broadcasting pages, we were creating mutable StrideArrays without suffering memory allocations, just like with the immutable StaticArrays.SArray type. The mutable StaticArrays.MArray, on the other hand, would have allocated:","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"julia> @benchmark sum(exp.(@StrideArray randn(8,10))) # PaddedMatrices\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     127.557 ns (0.00% GC)\n  median time:      127.986 ns (0.00% GC)\n  mean time:        128.116 ns (0.00% GC)\n  maximum time:     165.890 ns (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     888\n\njulia> @benchmark sum(exp.(@MMatrix randn(8,10))) # StaticArrays\nBenchmarkTools.Trial:\n  memory estimate:  672 bytes\n  allocs estimate:  1\n  --------------\n  minimum time:     703.599 ns (0.00% GC)\n  median time:      862.130 ns (0.00% GC)\n  mean time:        887.160 ns (4.56% GC)\n  maximum time:     136.675 μs (99.29% GC)\n  --------------\n  samples:          10000\n  evals/sample:     142","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"This is achieved thanks to a convenient macro, PaddedMatrices.@gc_preserve. When the macro is applied to a function call, it GC.@preserves all the arrays, and substitutes them with PtrArrays. This will safely preserve the array's memory during the call, while promising that the array won't escape, so that it may be stack allocated. Otherwise, passing mutable structs to non-inlined functions currently forces heap allocation. Many functions are overloaded for StrideArrays to provide a @gc_preserve barrier, so that calling them will not force heap allocation. However, doing this systematically is still a work in progress, so please file an issue if you encounter a function commonly used on arrays, especially if already defined in PaddedMatrices.jl, in which this is not the case.","category":"page"},{"location":"stack_allocation/","page":"Stack Allocattion","title":"Stack Allocattion","text":"When writing code making use of statically sized StrideArrays, you can use @gc_preserve in your own code when you can promise the array won't escape to make use of mutable stack allocated arrays. Note that @gc_preserve should also work on MArrays.","category":"page"},{"location":"#PaddedMatrices.jl","page":"Home","title":"PaddedMatrices.jl","text":"","category":"section"},{"location":"#Manual-Outline","page":"Home","title":"Manual Outline","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n\t\"getting_started.md\",\n\t\"arches/cascadelake.md\",\n\t\"arches/tigerlake.md\",\n\t\"arches/haswell.md\",\n\t\"rng.md\",\n\t\"broadcasting.md\",\n\t\"stack_allocation.md\"\n]\nDepth = 1","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [PaddedMatrices]","category":"page"},{"location":"#PaddedMatrices.jmul!-Union{Tuple{nc}, Tuple{kc}, Tuple{mc}, Tuple{Tb}, Tuple{Ta}, Tuple{Tc}, Tuple{AbstractArray{Tc,2},AbstractArray{Ta,2},AbstractArray{Tb,2},Any,Any,StaticInt{mc},StaticInt{kc},StaticInt{nc}}, Tuple{AbstractArray{Tc,2},AbstractArray{Ta,2},AbstractArray{Tb,2},Any,Any,StaticInt{mc},StaticInt{kc},StaticInt{nc},Any}} where nc where kc where mc where Tb where Ta where Tc","page":"Home","title":"PaddedMatrices.jmul!","text":"jmul!(C, A, B[, α = 1, β = 0])\n\nCalculates C = α * (A * B) + β * C in place.\n\nA single threaded matrix-matrix-multiply implementation. Supports dynamically and statically sized arrays.\n\nOrganizationally, jmul! checks the arrays properties to try and dispatch to an appropriate implementation. If the arrays are small and statically sized, it will dispatch to an inlined multiply.\n\nOtherwise, based on the array's size, whether they are transposed, and whether the columns are already aligned, it decides to not pack at all, to pack only A, or to pack both arrays A and B.\n\n\n\n\n\n","category":"method"},{"location":"#PaddedMatrices.jmulpackAB!-Union{Tuple{nc}, Tuple{kc}, Tuple{mc}, Tuple{T}, Tuple{AbstractArray{T,2},AbstractArray{T,2} where T,AbstractArray{T,2} where T,Any,Any,StaticInt{mc},StaticInt{kc},StaticInt{nc}}, Tuple{AbstractArray{T,2},AbstractArray{T,2} where T,AbstractArray{T,2} where T,Any,Any,StaticInt{mc},StaticInt{kc},StaticInt{nc},Any}} where nc where kc where mc where T","page":"Home","title":"PaddedMatrices.jmulpackAB!","text":"Packs both arrays A and B. Primitely packs both A and B into column major temporaries.\n\nColumn-major B is preferred over row-major, because without packing the stride across k iterations of B becomes excessive, and without nᵣ being a multiple of the cacheline size, we would fail to make use of 100% of the loaded cachelines. Unfortunately, using column-major B does mean that we are starved on integer registers within the macrokernel.\n\nOnce LoopVectorization adds a few features to make it easy to abstract away tile-major memory layouts, we will switch to those, probably improving performance for larger matrices.\n\n\n\n\n\n","category":"method"},{"location":"#PaddedMatrices.jmulpackAonly!-Union{Tuple{nc}, Tuple{kc}, Tuple{mc}, Tuple{Tb}, Tuple{Ta}, Tuple{Tc}, Tuple{AbstractArray{Tc,2},AbstractArray{Ta,2},AbstractArray{Tb,2},Any,Any,StaticInt{mc},StaticInt{kc},StaticInt{nc}}, Tuple{AbstractArray{Tc,2},AbstractArray{Ta,2},AbstractArray{Tb,2},Any,Any,StaticInt{mc},StaticInt{kc},StaticInt{nc},Any}} where nc where kc where mc where Tb where Ta where Tc","page":"Home","title":"PaddedMatrices.jmulpackAonly!","text":"Only packs A. Primitively does column-major packing: it packs blocks of A into a column-major temporary.\n\n\n\n\n\n","category":"method"},{"location":"#PaddedMatrices.@gc_preserve-Tuple{Any}","page":"Home","title":"PaddedMatrices.@gc_preserve","text":"@gc_preserve foo(A, B, C)\n\nApply to a single, non-nested, function call. It will GC.@preserve all the arguments, and substitute suitable arrays with PtrArrays. This has the benefit of potentially allowing statically sized mutable arrays to be both stack allocated, and passed through a non-inlined function boundary.\n\n\n\n\n\n","category":"macro"},{"location":"rng/#Random-Number-Generation","page":"Random Number Generation","title":"Random Number Generation","text":"","category":"section"},{"location":"rng/","page":"Random Number Generation","title":"Random Number Generation","text":"Randomly generating StrideArrays is fast, and can be done via a convenient macro:","category":"page"},{"location":"rng/","page":"Random Number Generation","title":"Random Number Generation","text":"julia> using PaddedMatrices, StaticArrays, BenchmarkTools\n\njulia> @btime sum(@StrideArray randn(8,10)) # PaddedMatrices\n  103.613 ns (0 allocations: 0 bytes)\n18.015335007499978\n\njulia> @btime sum(@SMatrix randn(8,10)) # StaticArrays\n  297.042 ns (0 allocations: 0 bytes)\n-4.091586809768035\n\njulia> @btime sum(@StrideArray rand(8,10)) # PaddedMatrices\n  18.862 ns (0 allocations: 0 bytes)\n43.61560492320911\n\njulia> @btime sum(@SMatrix rand(8,10)) # StaticArrays\n  171.001 ns (0 allocations: 0 bytes)\n38.47263930206726","category":"page"}]
}
